[{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/","title":"AWS Fargate Workshop","tags":[],"description":"","content":"Serverless Container Orchestration with AWS Fargate Overview In this hands-on workshop, you will build a serverless microservices system using AWS Fargate to deploy and manage containers without managing the underlying infrastructure.\nTechnologies Used AWS Fargate - Serverless compute engine for containers Amazon ECS - Container orchestration service Application Load Balancer - Traffic distribution and load balancing Amazon Aurora Serverless - Serverless database Amazon ECR - Container registry AWS CodePipeline - CI/CD automation Architecture Flow Workshop Goals Understand serverless container architecture on AWS Deploy microservices with Fargate and ECS Configure networking, load balancing, and auto-scaling Set up monitoring and CI/CD pipeline Main Content Introduction to Serverless Containers Prepare AWS Environment VPC and Networking Setup Create ECS Cluster with Fargate Deploy Microservices Configure Load Balancer Monitoring and Logging CI/CD Pipeline Resource Cleanup "},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/1-introduction/","title":"Introduction","tags":[],"description":"","content":"Content:\nWorkshop Goals What is Serverless Container Orchestration AWS Fargate Overview Learning Outcomes Workshop Goals This workshop aims to provide hands-on experience with serverless container orchestration using AWS Fargate. You will learn to build, deploy, and manage containerized microservices without managing the underlying infrastructure.\nKey Objectives:\nBuild a complete microservices architecture using AWS Fargate Implement container orchestration with Amazon ECS Configure networking, load balancing, and service discovery Set up monitoring, logging, and CI/CD pipelines Apply best practices for production-ready containerized applications What is Serverless Container Orchestration Serverless Container Orchestration combines the benefits of containerization with serverless computing principles:\nTraditional Container Orchestration:\nRequires managing EC2 instances, clusters, and scaling Manual infrastructure provisioning and maintenance Complex capacity planning and resource optimization Serverless Container Orchestration with Fargate:\nNo Infrastructure Management - AWS handles servers, patching, and scaling Pay-per-Use - Only pay for the compute resources your containers actually use Automatic Scaling - Scales up/down based on demand without manual intervention Built-in Security - Isolated compute environments with VPC integration Focus on Applications - Spend time on business logic, not infrastructure AWS Fargate Overview AWS Fargate is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS.\nCore Benefits:\nServerless: No EC2 instances to manage Secure: Task-level isolation with dedicated kernel runtime Scalable: Automatic scaling from zero to thousands of containers Cost-Effective: Pay only for the resources your containers use Integrated: Works seamlessly with AWS services (ALB, CloudWatch, IAM, etc.) Fargate vs EC2 Launch Type:\nFeature EC2 Launch Type Fargate Launch Type Infrastructure You manage EC2 instances AWS manages infrastructure Scaling Manual/ASG configuration Automatic task-level scaling Pricing EC2 instance hours vCPU and memory per second Maintenance OS patching, updates Fully managed Control Full instance control Container-focused Use Case Long-running, predictable workloads Variable, event-driven workloads Learning Outcomes By the end of this workshop, you will be able to:\nInfrastructure \u0026amp; Networking:\nDesign and implement VPC architecture for containerized applications Configure Application Load Balancer with target groups and health checks Set up service discovery and inter-service communication Container Orchestration:\nCreate and manage ECS clusters with Fargate launch type Write task definitions and service configurations Implement auto-scaling policies for containerized services Configure resource allocation (CPU, memory) optimization Application Development:\nContainerize applications using Docker best practices Build microservices with proper separation of concerns Implement health checks and graceful shutdown patterns Handle configuration management and secrets DevOps \u0026amp; Monitoring:\nSet up CI/CD pipelines for containerized applications Implement comprehensive monitoring and alerting Configure centralized logging with structured logs Use distributed tracing for debugging microservices Security \u0026amp; Best Practices:\nImplement least-privilege IAM roles and policies Configure network security with security groups and NACLs Manage secrets and environment variables securely Apply container security best practices Real-World Applications:\nE-commerce Platforms: Scalable product catalogs, order processing, payment systems API Backends: RESTful APIs with automatic scaling based on traffic Data Processing: Event-driven data pipelines and batch processing jobs Microservices Migration: Modernizing monolithic applications Startup MVPs: Rapid prototyping without infrastructure overhead Prerequisites: Basic understanding of containers and Docker Familiarity with AWS core services (VPC, EC2, IAM) Experience with at least one programming language (Python, Node.js, or Go) AWS CLI configured with appropriate permissions Estimated Duration: 4-5 hours Cost Estimate: $5-10 USD for the entire workshop (remember to clean up resources!) "},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/2-prepare-environment/","title":"Environment Setup","tags":[],"description":"","content":"Contents:\nRequirements List Set up AWS Account Install AWS CLI Install Docker Desktop Install Git Set up Code Editor Verify Installation Sample Application Requirements List Before starting this workshop, make sure you have the following:\nAWS account with Administrator privileges AWS CLI installed and configured Docker Desktop installed and running Git installed Text editor (VS Code recommended) Basic understanding of containers and AWS services Important: This workshop will create AWS resources that may incur charges.\nEstimated cost: $5-10 USD for the entire workshop.\nRemember to clean up resources after completion!\nSet up AWS Account AWS Account Requirements You need an AWS account with the following:\nA valid credit card attached for billing Verified phone number Administrator access or equivalent permissions Cost Estimation Service Estimated Cost/Hour Notes Fargate Tasks $0.04 - $0.08 3 services, 0.25 vCPU, 0.5GB RAM each Application Load Balancer $0.025 Standard ALB pricing Aurora Serverless v2 $0.06 - $0.12 Depends on usage NAT Gateway $0.045 Data processing charges CloudWatch Logs $0.50/GB Minimum expected logs ECR Storage $0.10/GB/month Container images Estimated Total ~$0.25/hour ~$1.00 for 4-hour workshop Create IAM User Instead of using the root account, create a dedicated IAM user for this workshop.\nLog in to AWS Console → IAM Users → Create user User details:\nUser name: fargate-workshop-user Provide user access to AWS Management Console Set permissions:\nAttach policies directly Search and select: AdministratorAccess Review and create Download credentials or copy Access Key ID and Secret Access Key Install AWS CLI Download AWS CLI MSI installer: https://awscli.amazonaws.com/AWSCLIV2.msi Run the MSI file and follow the installation instructions Open a new Command Prompt and verify: aws --version\nExpected result: aws-cli/2.15.30 Python/3.11.8 Windows/10 exe/AMD64 prompt/off Verify installation: aws \u0026ndash;version Configure AWS CLI aws configure AWS Access Key ID [None]: AKIA\u0026hellip; AWS Secret Access Key [None]: wJalrXUt\u0026hellip; Default region name [None]: us-east-1 Default output format [None]: json Verify your AWS CLI configuration\nCheck your identity: aws sts get-caller-identity\nCheck permissions: aws s3 ls\nCheck configured region: aws configure get region Expected aws sts get-caller-identity output:\n{ \u0026ldquo;UserId\u0026rdquo;: \u0026ldquo;AIDACKCEVSQ6C2EXAMPLE\u0026rdquo;, \u0026ldquo;Account\u0026rdquo;: \u0026ldquo;123456789012\u0026rdquo;, \u0026ldquo;Arn\u0026rdquo;: \u0026ldquo;arn:aws:iam::123456789012:user/fargate-workshop-user\u0026rdquo; }\nInstall Docker Desktop Download Docker Desktop: https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe\nRun the installer and follow setup instructions\nRestart your computer after installation\nLaunch Docker Desktop from Start Menu\nVerify installation:\ndocker \u0026ndash;version docker run hello-world\nVerify Docker is Working Check Docker version: docker \u0026ndash;version\nTest Docker with hello-world: docker run hello-world\nCheck Docker Compose: docker compose version Expected hello-world output: Hello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\nThe Docker client contacted the Docker daemon. The Docker daemon pulled the \u0026ldquo;hello-world\u0026rdquo; image from the Docker Hub. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. Install Git Download Git: https://git-scm.com/download/win Run the installer with default settings\nVerify installation: git \u0026ndash;version Basic Git configuration: git config \u0026ndash;global user.name \u0026ldquo;Your Name\u0026rdquo; git config \u0026ndash;global user.email \u0026ldquo;email@example.com\u0026rdquo;\nCheck your configuration: git config \u0026ndash;list\nSet up Code Editor VS Code (Recommended): https://code.visualstudio.com/\nInstall useful extensions:\nAWS Toolkit – AWS services integration\nDocker – Manage Docker containers\nYAML – Syntax highlighting for YAML\nJSON – JSON formatting\nGitLens – Advanced Git features\nPython – Python support\nGo – Go language support\nVerify Installation Run the following commands to make sure everything is set up correctly: AWS CLI: aws \u0026ndash;version aws sts get-caller-identity\nDocker: docker \u0026ndash;version docker run hello-world\nGit: git \u0026ndash;version\nCheck AWS region: aws configure get region\nIf all the above commands run successfully, you are ready for the workshop!\n"},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/3-vpc-and-networking-setup/","title":"VPC and Networking Setup","tags":[],"description":"","content":"Contents:\nObjectives Create VPC Create Subnets Create Internet Gateway Create NAT Gateway Create Route Tables Create Security Groups Expected Result Objectives Create a dedicated VPC for the application. Split Public Subnets and Private Subnets across 2 Availability Zones (Multi-AZ). Set up Internet Gateway, NAT Gateway, and Route Tables to ensure: Public Subnets can access the Internet (for ALB, NAT). Private Subnets can access the Internet via NAT Gateway (for Fargate Tasks to pull images, call APIs, etc.). Prepare Security Groups to control network traffic. Create VPC Go to VPC Console → Create VPC Choose: Name tag: fargate-workshop-vpc IPv4 CIDR: 10.0.0.0/16 Tenancy: Default Create Subnets Go to Subnets → Create subnet Select the newly created VPC Create 4 subnets: Public Subnet 1: 10.0.1.0/24 (AZ1) Private Subnet 1: 10.0.3.0/24 (AZ1) Public Subnet 2: 10.0.2.0/24 (AZ2) Private Subnet 2: 10.0.4.0/24 (AZ2) Create Internet Gateway Go to Internet Gateways → Create IGW Name: fargate-igw Attach it to the newly created VPC Create NAT Gateway Go to NAT Gateways → Create NAT Gateway Select: Public Subnet 1 (AZ1) Allocate Elastic IP Create a second NAT Gateway for Public Subnet 2 (AZ2) for High Availability Create Route Tables Public Route Table: Route: 0.0.0.0/0 → Internet Gateway Associate with Public Subnet 1 \u0026amp; 2 Private Route Table AZ1: Route: 0.0.0.0/0 → NAT Gateway AZ1 Associate with Private Subnet 1 Private Route Table AZ2: Route: 0.0.0.0/0 → NAT Gateway AZ2 Associate with Private Subnet 2 Create Security Groups ALB Security Group: Inbound: TCP 80, 443 from 0.0.0.0/0 Outbound: Allow all Fargate Tasks Security Group: Inbound: TCP 8080 from ALB SG Outbound: Allow all Database Security Group (if using Aurora/DynamoDB in Private Subnet): Inbound: 3306 (Aurora) from Fargate SG Outbound: Allow all Expected Result You should now have a network structure with:\nVPC 10.0.0.0/16 2 Public Subnets (AZ1, AZ2) + 2 Private Subnets (AZ1, AZ2) Internet Gateway for Public Subnets NAT Gateway for Private Subnets Properly configured Route Tables Separate Security Groups for ALB, Fargate, and Database "},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/4-create-ecs-cluster-with-fargate/","title":"Create ECS Cluster with Fargate","tags":[],"description":"","content":"Contents:\nLogin to ECR Create ECR Repository Build and Push Image to ECR Create ECS Cluster Create Task Definition Deploy Service with Fargate Login to ECR Run the following command to log in to ECR:\naws ecr get-login-password \u0026ndash;region us-east-1 | docker login \u0026ndash;username AWS \u0026ndash;password-stdin \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com\nCreate ECR Repository Open ECR Console → Create repository Enter repository name: fargate-workshop-app Select Private Create the repository and copy the URI Build and Push Image to ECR Build the Docker image:\ndocker build -t fargate-workshop-app .\nTag the image with your ECR URI (replace \u0026lt;AWS_ACCOUNT_ID\u0026gt; with your account ID):\ndocker tag fargate-workshop-app:latest \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/fargate-workshop-app:latest\nPush the image to ECR:\ndocker push \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/fargate-workshop-app:latest\nCreate ECS Cluster Open ECS Console → Clusters → Create Cluster Select Networking only (Powered by AWS Fargate) Enter cluster name: fargate-workshop-cluster Choose the VPC and Subnets created in the Networking section Click Create Create Task Definition Open ECS Console → Task Definitions → Create new Task Definition Select Fargate Enter task name: fargate-workshop-task Select Task Role: ecsTaskExecutionRole Define the container: Container name: fargate-workshop-app Image: \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/fargate-workshop-app:latest Port mappings: 8080 (TCP) Click Next step and Create Deploy Service with Fargate Open ECS Console → Select cluster fargate-workshop-cluster → Create Service Configure: Launch type: Fargate Task Definition: fargate-workshop-task Platform version: LATEST Service name: fargate-service Number of tasks: 2 Networking: Select VPC and Subnets Assign public IP: ENABLED (for public access) Choose a Security Group that allows port 8080 Load Balancing (optional): Select Application Load Balancer Create a new target group Listener port: 80 Review and Create Service Once the service is running, you can access the application via the DNS name of the Load Balancer.\n"},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/5-deploying-microservices/","title":"Deploying Microservices","tags":[],"description":"","content":"Contents:\nOverview Create Task Definitions for each service Deploy services to ECS Cluster Verify deployment Overview In this section, we will deploy multiple container services (microservices) to an ECS Cluster using AWS Fargate.\nEach service will handle a specific function such as an API service, a frontend service, or a background worker.\nAdvantages of a microservices approach:\nImproved scalability for individual components Easier maintenance and upgrades without affecting the entire system Reduced risk if a single service fails Create Task Definitions for each service Service 1 – API\nGo to ECS Console → Task Definitions → Create new Task Definition Select Fargate Name: api-service-task Select Task Role: ecsTaskExecutionRole Add container: Name: api-service Image: \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/api-service:latest Port mappings: 8080 Save and create the Task Definition. Service 2 – Frontend\nSimilar to Service 1, but: Name: frontend-service Image: \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/frontend-service:latest Port mappings: 80 Service 3 – Worker (Background Job)\nSimilar, but: Name: worker-service Image: \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/worker-service:latest No port mapping required if not exposed externally Deploy services to ECS Cluster Deploy API Service\nGo to ECS Console → Select your Cluster → Create Service Launch type: Fargate Task Definition: api-service-task Service name: api-service Desired tasks: 2 Networking: Select VPC and Subnets Enable Assign Public IP if public access is needed Choose Security Group that allows port 8080 (Optional) Attach to Application Load Balancer target group Deploy Frontend Service\nSame as API Service, but: Task Definition: frontend-service-task Port: 80 Security Group allows port 80 Deploy Worker Service\nSimilar, but: Task Definition: worker-service-task No ALB attachment Runs in private subnet Verify deployment Go to ECS Console → Cluster → Services tab Ensure service status is Running If service is attached to an ALB: Go to EC2 Console → Load Balancers\nCopy the ALB DNS name\nTest from browser or use:\ncurl http://\u0026lt;ALB_DNS_NAME\u0026gt;/health\nExpected result: You should receive a 200 OK response or a valid API message.\n"},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/6-configure-load-balancer/","title":"Configure Load Balancer","tags":[],"description":"","content":"Contents:\nOverview Create an Application Load Balancer (ALB) Create Target Groups Attach Services to Target Groups Test the Setup Overview The Application Load Balancer (ALB) will distribute incoming traffic to multiple services (microservices) running on ECS Fargate.\nALB supports path-based routing and host-based routing, allowing requests to be directed to the correct service.\nBenefits:\nLoad balances traffic across multiple service instances Ensures high availability and fault tolerance Easily scales to handle increased traffic Create an Application Load Balancer (ALB) Go to EC2 Console → Load Balancers → Create Load Balancer Select Application Load Balancer Name: fargate-alb Scheme: Internet-facing (for public access) IP address type: IPv4 Select your VPC and at least 2 subnets in different Availability Zones (Multi-AZ) Security Group: Allow HTTP (80) and HTTPS (443) if using SSL Skip Target Group creation for now (will create later) Click Create Load Balancer Create Target Groups Go to EC2 Console → Target Groups → Create target group Target type: IP addresses Name: api-target-group Protocol: HTTP, Port: 8080 (or your API service port) Select your VPC Health check path: /health Create an additional target group for the Frontend: Name: frontend-target-group Port: 80 Health check path: / Attach Services to Target Groups Go to ECS Console → Cluster → Service → Select the service you want to configure Edit the service → In the Load balancing section: Select Application Load Balancer Listener: HTTP 80 (or HTTPS 443 if SSL is enabled) Choose the corresponding Target Group (API → api-target-group, Frontend → frontend-target-group) Save and redeploy the service Test the Setup Go to EC2 Console → Load Balancers → Select fargate-alb Copy the ALB DNS name Access it in your browser: http://\u0026lt;ALB_DNS\u0026gt;/ → should display the frontend http://\u0026lt;ALB_DNS\u0026gt;/api/health → should return the API health check response Ensure that the Target Group status shows healthy "},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/7-monitoring-and-logging/","title":"Monitoring and Logging","tags":[],"description":"","content":"Contents:\nOverview Enable Amazon CloudWatch Logs Enable Container Insights Viewing Logs and Metrics Integrating Alerts (Alarms) Overview Monitoring and logging are essential for tracking the health and performance of your application and infrastructure.\nIn this workshop, we will use:\nAmazon CloudWatch Logs to store and analyze logs from containers. CloudWatch Container Insights to monitor CPU, Memory, Network, and Storage usage. CloudWatch Alarms to receive notifications when issues occur. Enable Amazon CloudWatch Logs Go to ECS Console → Cluster → Select your service. In the Task Definition, check the log driver configuration: Select awslogs as the log driver. Log Group name: /ecs/fargate-workshop Region: us-east-1 (or your chosen AWS region). Save and redeploy the service. Enable Container Insights Go to CloudWatch Console → Container Insights. Select Enable for your ECS Cluster. Container Insights will automatically collect: CPUUtilization MemoryUtilization NetworkRxBytes / NetworkTxBytes StorageReadBytes / StorageWriteBytes Viewing Logs and Metrics Open CloudWatch Console → Logs → Select the Log Group /ecs/fargate-workshop. View container logs (stdout, stderr). Go to CloudWatch Metrics → ECS → Cluster Metrics to see CPU, RAM, and Network statistics. Filter by specific services or tasks for detailed analysis. Integrating Alerts (Alarms) Go to CloudWatch Console → Alarms → Create alarm. Select a metric, for example: ECS/ContainerInsights – CPUUtilization. Set a threshold, e.g., CPU \u0026gt; 80% for 5 minutes. Choose an SNS Topic to receive email notifications when the alarm is triggered. Save and activate the alarm. Expected Outcome:\nYou can view logs for each container to debug issues. Monitor application performance in real-time. Receive notifications when the system encounters issues or exceeds resource thresholds. "},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/8-ci-cd-pipeline/","title":"CI/CD Pipeline","tags":[],"description":"","content":"Contents:\nOverview Create a CodeCommit repository Create a CodeBuild project buildspec.yml (example) Create a CodePipeline pipeline Test the pipeline Troubleshooting \u0026amp; tips Overview This section shows how to implement a CI/CD pipeline that builds Docker images and deploys to ECS Fargate automatically. We will use:\nAWS CodeCommit (source) AWS CodeBuild (build \u0026amp; push image to ECR) Amazon ECR (container registry) AWS CodePipeline (orchestration) Amazon ECS (deploy) Create a CodeCommit repository Open AWS CodeCommit Console → Create repository.\nRepository name: fargate-microservices.\nNote the repository HTTPS/SSH clone URL. Example (HTTPS):\ngit clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/fargate-microservices\nAdd your application source code (Dockerfile, app code, buildspec.yml — see below) and push to the repository.\nCreate a CodeBuild project Open AWS CodeBuild Console → Create build project. Project name: fargate-build. Source: choose AWS CodeCommit and pick the repository fargate-microservices. Environment: Environment image: Managed image aws/codebuild/standard:7.0 (or latest). Operating system: (default) Runtime: (default) Enable Privileged (required for Docker build/push). Service role: You can let CodeBuild create a new service role, or attach an existing role that has permissions: AmazonEC2ContainerRegistryPowerUser (or granular ECR push permissions) CloudWatch Logs write permissions CodeBuild basic execution Buildspec: either use the buildspec file in the repo (recommended) or paste inline. Save the project and run a test build after you push code. buildspec.yml (example) Create a file named buildspec.yml in the root of your repository. Example (keep indentation exactly as below):\nversion: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com - REPOSITORY_URI=\u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/fargate-microservice - IMAGE_TAG=latest build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $REPOSITORY_URI:$IMAGE_TAG . - docker tag $REPOSITORY_URI:$IMAGE_TAG $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $REPOSITORY_URI:$IMAGE_TAG - printf '[{\u0026quot;name\u0026quot;:\u0026quot;fargate-microservice\u0026quot;,\u0026quot;imageUri\u0026quot;:\u0026quot;%s\u0026quot;}]' $REPOSITORY_URI:$IMAGE_TAG \u0026gt; imagedefinitions.json artifacts: files: - imagedefinitions.json Notes:\nReplace \u0026lt;ACCOUNT_ID\u0026gt; with your AWS account ID. imagedefinitions.json is needed by CodePipeline ECS deploy action. Use IMAGE_TAG strategy that fits you (e.g., using commit SHA). Create a CodePipeline pipeline Open AWS CodePipeline Console → Create pipeline.\nPipeline name: fargate-cicd-pipeline.\nService role: let CodePipeline create new role or use existing with proper permissions.\nAdd stages:\nSource stage\nProvider: AWS CodeCommit Repository: fargate-microservices Branch: e.g., main Build stage\nProvider: AWS CodeBuild Project: fargate-build (created above) Ensure build outputs the artifact imagedefinitions.json Deploy stage\nProvider: Amazon ECS Action mode: Deploy Cluster name: your ECS cluster (e.g., fargate-workshop-cluster) Service name: your ECS service (e.g., api-service or fargate-service) Image definitions file: imagedefinitions.json Create the pipeline. The pipeline will run automatically for the current commit.\nTest the pipeline Make a small change in your repository (e.g., update a README or bump version).\nCommit \u0026amp; push to CodeCommit:\ngit add . git commit -m \u0026ldquo;Test pipeline\u0026rdquo; git push origin main\nCodePipeline should trigger automatically:\nSource pulls commit CodeBuild builds and pushes image to ECR CodePipeline deploys new image to ECS service using imagedefinitions.json Verify:\nCheck CodePipeline execution history Check CodeBuild logs for errors Check ECS service: tasks should update and show the new image Test application endpoint (ALB DNS) Troubleshooting \u0026amp; tips Permissions errors: ensure CodeBuild role has ECR push permission and CloudWatch Logs write permission. Privileged mode: required for docker build inside CodeBuild. Build timeout: adjust timeouts in CodeBuild settings if builds take long. imagedefinitions.json format: must exactly match [{ \u0026quot;name\u0026quot;: \u0026quot;\u0026lt;container-name-in-taskdef\u0026gt;\u0026quot;, \u0026quot;imageUri\u0026quot;: \u0026quot;\u0026lt;uri\u0026gt;\u0026quot; }]. Multiple services: if pipeline deploys multiple services, either create separate deploy actions or separate pipelines per service. Local testing: test Docker build and push locally to ensure Dockerfile works before pipeline run. Expected outcome: committing code to the repository triggers an automated build, image push to ECR, and deployment to your ECS Fargate service.\n"},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/9-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Contents:\nWhy Clean Up? Delete ECS Services and Cluster Delete Load Balancer and Target Groups Delete ECR Repositories Delete VPC and Networking Components Delete CodePipeline, CodeBuild, and CodeCommit Delete IAM User and Permissions Check Remaining Costs Why Clean Up? This workshop has created several AWS resources that can continue to incur costs if not removed, such as ECS Clusters, ALBs, NAT Gateways, ECR Storage, and CloudWatch Logs.\nCleaning up ensures:\nAvoiding unexpected charges. Keeping your AWS environment tidy. Preventing resource exposure or misuse. Delete ECS Services and Cluster Open Amazon ECS Console → Select Cluster: fargate-workshop-cluster. Stop all Services: Select each service → Delete service → Confirm. Wait for all Tasks to stop. Delete the Cluster: Delete cluster. Delete Load Balancer and Target Groups Open EC2 Console → Load Balancers. Select the ALB created (e.g., fargate-workshop-alb) → Delete. Open Target Groups → Delete groups linked to the ALB. Delete ECR Repositories Open Amazon ECR Console → Repositories. Select the repository (e.g., fargate-microservices) → Delete. Select Force delete if images are present inside. Delete VPC and Networking Components Only delete if it was created specifically for this workshop. Do not delete the default VPC.\nOpen VPC Console → Your VPCs → Select the workshop VPC → Delete. Remove related Subnets, Route Tables, Internet Gateway, NAT Gateway, and Security Groups. Delete CodePipeline, CodeBuild, and CodeCommit CodePipeline: Open the console → Select the pipeline → Delete. CodeBuild: Remove unused build projects. CodeCommit: Delete the workshop repository if code retention is not needed. Delete IAM User and Permissions Open IAM Console → Users → Select fargate-workshop-user. Remove Access Keys. Delete the user completely. Check Remaining Costs Open Billing \u0026amp; Cost Management. Select Cost Explorer to review daily costs. Ensure no services are running unexpectedly. Expected Outcome:\nAll AWS resources created during the workshop are deleted, your AWS account is back to a clean state, and no further charges are incurred.\n"},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://trungquangnguyeen.github.io/Serverless-Container-Orchestration/tags/","title":"Tags","tags":[],"description":"","content":""}]